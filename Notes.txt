5/16/19
This is to act as a central location for progress and work done, as well as to track what specifically has been run/produced when and by what notebook/code.
Firstly, I will attempt to summarize recent work done prior to the creation of this record.
In the most recent DSFP I learned that pandas export to csv does not preserve bitwise data and so all of the calculated features were not stored properly. This led to modifying the feature calculation to output to a pickle instead. In addition to this modification, a great deal of effort was expended updating the methods used by the feature calculation to improve performance both in terms of speed and memory utilization. In the process, I discovered that there were several inconsistent feature definitions. Some modified definitions affected only a handful of lightcurves, some affect all lightcurves. All features have been recalculated for every lightcurve in every quarter. A quarter takes around an hour to process now on 16 cores. For some reason 16 cores works faster than more or less. 
The troubleshooting for the feature calculations was done in several different notebooks. The final definitions and methods are implemented in keplerml.py and no other notebook should be assumed to represent the final work.  For reference, however, the primary notebook for making improvments was speedup_sandbox.ipynb which contains some record of the improvements made and the reasoning.

All previous dimensionality reductions, scoring, and outlier labelling will be superceded by future work, as all previous work was based on slightly different, not rigorously preserved, features. 

keplerml.py - features updated to have more consistent definitions, saving mechanism switched to pickle dataframes instead of outputing dataframes to csv files, performance improvements using numba jit, progress is better detailed in output statements (if verbose is True) and failsafe mechanisms have been improved. 

clusterOutliers.py - modified to accept both pickle and csv input data, some functionality moved to quarterTools.py, changed save method to output whole clusterOutliers object to a pickled object (using the file extension .coo for pickled cluster outlier objects). Moved pca and tsne reduction methods to quarterTools.py, moved bulk of interactive plot to quarterTools.py, modified plot method to call interactive_plot from quarterTools.py with data and some default options from the clusterOutlier object

quarterTools.py - added pca and tsne reduction methods and interactive plotting from clusterOutliers.py

5/17/19
Began updating PCA Reductions.ipynb to utilize updated python scripts and to reprocess updated features.
Tried some visualization of scoring on different PCA reductions, but ran into difficulties given the quantized nature of the labels.

5/19/19
Cluster outlier objects are now the container for all analysis and data. All data and analysis should be saved to their respective quarters including reductions, outlier scores, and cluster labels when they're produced. A cluster outlier object cotains its own filepath (which can be changed) and a simple save() call after adding any attributes will preserve the new analysis. PCA reductions explaining 90/95/99% of the variance have been produced for all quarters based on the updated features and saved to the respective cluster outlier objects. Renamed the git branch form PCA_decomp to outlier_scoring to reflect the updated focus of the ongoing work. PCA work is essentially done, I believe it prudent to move forward using all PCA reductions for the present work as ground truth is unknown, but outlier overlap is significant.

PCA Reductions - Removed all previous work based on previous features, summarized what it used to contain at the head, and updated it to work with updated clusterOutliers and features. Now contains the code used to produce PCA reductions.

5/20/19
Testing a full scoring process on the full set of data, each of the PCA reductions. On each set of data this entails:
- Exact scoring using a range of different neighbors using kNN trained on FULL dataset
-- Can be accomplished in a single run by using kNN to calculate distances to the max neighbor
-- Arbitrarily choosing k=4 to 14
- Average scoring using kNN trained on full dataset
-- Can average output from exact scoring above

- Exact scoring using a range of different neighbors using kNN trained on a sample of the dataset
-- Sample size 10k
-- Averaging exact score over 10 runs
-- Same number of neighbors, k=4 to 14, this is probably probematic since the distances will be inherently larger...
- Average scoring using kNN trained on sample of dataset
-- can average exact scores from above

This will result in 22 scores for every reduction for every object. 10 based on exact distance to the exact k-th neighbor, 10 based on average exact distance to the k-th nearest sample point, 1 average of exact distances to exact k-th neighbors, and 1 average of average exact distances to k-th nearest sample point. There will be a total of 88 scores per object.

When plotting results:
In order to evaluate the results, I will sort the scores by the average score from the full dataset. I will choose a color gradient to represent the data and plot scoring method vs. point id. The data will be color coded to indicate score, visually representing the discrepencies between different scoring methods. This will be done on a per quarter basis.

Started implementing the above using quarter 1 data, got through all scoring for the quarter for the full dataset.
Started scoring for all quarters before leaving.

Still todo: score PCA reductions

5/21/19
Realized that the PCA reductions weren't saved to their cluster outlier files except for quarter 1 because of typos :-/, reran and saved PCA reductions. Followup: again didn't save, but finally managed to get them all saved to their respective files.

5/30/19
Didn't document the past weekish, oops. Either way, the main work that I've been doing over the past week is producing a ton of scores for the quarter 1 data and making plots to evaluate the differences in those scores.
A summary: 
exact scoring varies for near neighbors a bit, and evens out for higher neighbors (past 30 or so for Q1 data). Since higher neighbor scores resemble each other fairly strongly, so too do the average of these scores, the lower neighbor scores get drowned out fairly quickly. Scoring by further neighbors appears to inflate some scores.
Scoring against a sample of the data is equivalent to scoring against higher neighbors. Since the higher neighbors provide fairly stable scores, size of sample scored against and the number of iterations averaged over did not affect the scores significantly. 
PCA Reductions tend to increase the scores of middle points (with scores between 0.01 and 0.99)

6/1/19-7/28/19
DSFP 6/9-6/15, 
AbSciCon 6/23-6/29, 
moving efforts 7/6-7/28

Notes on things that happened somewhere in there:
Chose to move forward and score all quarters
Full feature-set, PCA90, PCA95, PCA99
    Exact 4th to 13th, Average
    Sampled 4th to 13th, Average
Note: Sample size is 10k, scores are an average of 10 iterations

Noticed at AbSciCon that scores between quarters are weird for Boyajain's star. The scores for less active quarters were higher than more active quarters. The defined scaling makes the most outlying object have a score of 1, regardless of what that is. Looking across quarters, the most outlying object varies wildly, usually some form of artifact but not a singular type of artifact. To account for this inconsistency, I tested the idea of including a standard reference and scaling to that. 

I generated a sinusoid for a quarter with a period of 1 day and an amplitude of 0.5 offset to 1. I calculated features for the sinuosoid and saved it as a reference, and inserted it into the quarter's data as though it were an object. I scored the data to a sample of 1000 points, and averaged over 10 iterations as per the previously detailed methodology. Finally, I divided all scores by the score of the sinusoid so that it would be defined as having a score of 1.

I followed up scoring all quarters this way for the full feature sets and the reductions.

7/29/19 
Created a method to plot the scores for an object in all quarters. It does not check whether an object is in all quarters, not sure how it would react to an object not contained in all quarters.

7/30-8/7
Writing...

8/7/19
Discovered that Q2 and Q3 unzipped improperly and dropped 15k and 60k lightcurves, respectively. Unzipped, generated filelists, replaced existing folders and filelists, calculated features for full lightcurves (did the whole quarter to get processing times).

8/8/19
Generating PCA reductions for Q2 and Q3, scoring Q2 and Q3