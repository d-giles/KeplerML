{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook previously sought to explore the utility of dimensionality reduction with PCA prior to clustering considering 3 levels of variance explained: 90%, 95%, and 99%. \n",
    "Aside from the reduction, the data was processed in the same way as done previously. Prior to dimensionality reduction, the was scaled using sklearn's standardScaler. After reduction, the data was clustered using the method clusterOutliers.db_out.\n",
    "These reductions were compared to the clustering on the full set of features from the paper as well as a rerun of the clustering on the full set of features (the rerun is hereafter referred to as the baseline) and a summary of these results is stored in the Google Sheets file 'PCA Reduction'.\n",
    "In short:\n",
    "Treating the paper data as the ground truth:\n",
    "Precision: 100%, Recall: 68%\n",
    "The paper data had a stricter definition of a cluster and contained essentially every non-core data point identified in the reductions and the rerun baseline. Almost every point identified as an outlier or an edge member in the reductions was identified as such in the paper's results. If we treat the paper as the ground truth (problematic) then the precision of the reductions is perfect.\n",
    "The reductions (and rerun baseline) clustered more of the data and identified between 63% and 70% of the non-core identified in the paper data, a recall (assuming the paper is the ground truth) of about 68%.\n",
    "\n",
    "Treating the new baseline (reclustering on the full set of features) as the ground truth:\n",
    "Precision: 90%-99%, recall: 85%-90%\n",
    "\n",
    "The reduced recall as compared to the paper is mitigated when examining against the re-clustered data. There is not a simple way to say exactly how good the results of the reduced data are since there is no simple way of saying what the ground truth should really be as there is not a simple way of defining what an outlier should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the custom code developed for this work\n",
    "import sys\n",
    "sys.path.append('python')\n",
    "from clusterOutliers import clusterOutliers as coo\n",
    "from clusterOutliers import import_gen\n",
    "import quarterTools as qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Calling import_gen with defaults, creates an importer for clusterOutlier objects w/ data \n",
    "stored in a common directory with a common naming convention.\n",
    "    path to data files directory (filedir): /home/dgiles/Documents/KeplerLCs/output/\n",
    "    output file suffix (suffix): _output.p\n",
    "    path to fits files directory (fitsdir): /home/dgiles/Documents/KeplerLCs/fitsFiles\n",
    "    output file extension (out_file_ext): .coo\n",
    "    \n",
    "This has been executed on 5/19/2019 to initialize the cluster outlier objects, this cell does not need to be run again\n",
    "unless the definition of the object is significantly altered as it will overwrite the existing object based only on the \n",
    "feature data. Doing this would overwrite any work or additions made to the cluster outlier object including \n",
    "dimensionality reductions, outlier scoring, and any other analysis stored in these objects. This cell has been\n",
    "converted to markdown to avoid accidentally running the code.\n",
    "\"\"\"\n",
    "```\n",
    "import_quarter = import_gen()\n",
    "# Q_dict contains a clusterOutlier object for each quarter.\n",
    "Q_dict = {'Q{}'.format(i):import_quarter('Q{}'.format(i)) for i in range(1,18)}\n",
    "for k in Q_dict:\n",
    "    Q_dict[k].save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used to import existing cluster outlier objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "Q_dict = dict()\n",
    "for i in range(1,18):\n",
    "    with open('/home/dgiles/Documents/KeplerLCs/output/Q{}.coo'.format(i),'rb') as file:\n",
    "        Q_dict['Q{}'.format(i)]=pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data using StandardScaler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda3/envs/KeplerFull37/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "python/quarterTools.py:90: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n",
      "  scaled_data = scaler.transform(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding minimum number of dimensions to explain 90.0% of the variance...\n",
      "\n",
      "        Dimensions: 18,\n",
      "        Variance explained: 90.2%\n",
      "        \n",
      "Scaling data using StandardScaler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda3/envs/KeplerFull37/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "python/quarterTools.py:90: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n",
      "  scaled_data = scaler.transform(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding minimum number of dimensions to explain 95.0% of the variance...\n",
      "\n",
      "        Dimensions: 24,\n",
      "        Variance explained: 95.3%\n",
      "        \n",
      "Scaling data using StandardScaler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgiles/anaconda3/envs/KeplerFull37/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "python/quarterTools.py:90: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n",
      "  scaled_data = scaler.transform(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding minimum number of dimensions to explain 99.0% of the variance...\n",
      "\n",
      "        Dimensions: 36,\n",
      "        Variance explained: 99.0%\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "pca90 = Q_dict['Q2'].pca_red(df='self',red_name='PCA90',var_rat=0.9,scaled=False,verbose=True)\n",
    "pca95 = Q_dict['Q2'].pca_red(df='self',red_name='PCA95',var_rat=0.95,scaled=False,verbose=True)\n",
    "pca99 = Q_dict['Q2'].pca_red(df='self',red_name='PCA99',var_rat=0.99,scaled=False,verbose=True)\n",
    "Q_dict['Q2'].save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Creating PCA reductions for each quarter that explain 90%, 95%, and 99% of variance\n",
    "\n",
    "Reductions have been produced and saved in their respective cluster outlier files, output has been suppressed, cell converted to markdown to avoid accidental execution.\n",
    "\"\"\"\n",
    "```\n",
    "for k in Q_dict.keys():\n",
    "    print(\"Starting \"+k)\n",
    "    pca90 = Q_dict[k].pca_red(df='self',red_name='PCA90',var_rat=0.9,scaled=False,verbose=False)\n",
    "    pca95 = Q_dict[k].pca_red(df='self',red_name='PCA95',var_rat=0.95,scaled=False,verbose=False)\n",
    "    pca99 = Q_dict[k].pca_red(df='self',red_name='PCA99',var_rat=0.99,scaled=False,verbose=False)\n",
    "    Q_dict[k].save()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
