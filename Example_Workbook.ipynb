{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "import fnmatch\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2TkAgg\n",
    "from matplotlib import colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.gridspec as gridspec\n",
    "from multiprocessing import Pool,cpu_count\n",
    "from numbapro import cuda\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "import os\n",
    "import pyfits\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    import Tkinter as Tk\n",
    "else:\n",
    "    import tkinter as Tk\n",
    "    \n",
    "from tkFileDialog import askopenfilename,askdirectory,asksaveasfile\n",
    "\n",
    "import keplerml\n",
    "import km_outliers\n",
    "import db_outliers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============ Start ============\n",
    "\"\"\"\n",
    "\n",
    "# User defined\n",
    "featCSV = \"/home/dgiles/Documents/KeplerLCs/output/Q8_data.csv\"\n",
    "fitsDir = \"/home/dgiles/Documents/KeplerLCs/fitsFiles/Q8fitsfiles\"\n",
    "numLCs = 5000\n",
    "\n",
    "class feature_importer(object):\n",
    "    def __init__(self,feats,fitsDir):\n",
    "        self.data = pd.read_csv(feats,index_col=0)\n",
    "        self.fitsDir = fitsDir\n",
    "        self.files = self.data.index\n",
    "        \n",
    "    def poolRKC(self):\n",
    "        numcpus = cpu_count()\n",
    "        p = Pool(numcpus)\n",
    "        lcs = p.imap(keplerml.read_kepler_curve,self.filesSample)\n",
    "        p.close()\n",
    "        p.join\n",
    "        return lcs\n",
    "    \n",
    "    def randSample(self, numLCs):\n",
    "        \"\"\"\n",
    "        Returns a random sample of numLCs light curves, data returned as an array\n",
    "        of shape [numLCs,3,len(t)]\n",
    "        Rerunning this, or randSampleWTabby will replace the previous random sample.\n",
    "        \"\"\"\n",
    "        self.numLCs = numLCs\n",
    "        print(\"Creating random file list...\")\n",
    "        self.dataSample = self.data.sample(n=numLCs)\n",
    "        self.filesSample = self.dataSample.index\n",
    "        print(\"Importing lightcurves...\")\n",
    "        \n",
    "        return poolRKC()\n",
    "    \n",
    "    def randSampleWTabby(self, numLCs):\n",
    "        \"\"\"\n",
    "        Returns a random sample of numLCs light curves, data returned as an array\n",
    "        of shape [numLCs,3,len(t)]\n",
    "        Rerunning this, or randSample will replace the previous random sample.\n",
    "        \"\"\"\n",
    "        self.numLCs = numLCs\n",
    "        print(\"Creating random file list...\")\n",
    "        self.dataSample = self.data.sample(n=numLCs)\n",
    "\n",
    "        print(\"Checking for Tabby...\")\n",
    "        if not dataSample.index.str.contains('8462852').all():\n",
    "            print(\"Adding Tabby...\")\n",
    "            self.dataSample.drop(self.dataSample.index[0])\n",
    "            self.dataSample.append(self.data[self.data.index.str.contains('8462852')])\n",
    "        self.filesSample = self.dataSample.index\n",
    "        print(\"Importing lightcurves...\")\n",
    "        return poolRKC()\n",
    "    \n",
    "    def fullQ(self):\n",
    "        self.filesSample = self.files\n",
    "        self.dataSample = self.data\n",
    "        return poolRKC()\n",
    "    \n",
    "    def tsne_fit(self,perplexity=len(self.dataSample)/3):\n",
    "        \"\"\"\n",
    "        Performs a t-SNE dimensionality reduction on the data sample generated.\n",
    "        Uses a PCA initialization and the perplexity given, or defaults to 50.\n",
    "        \n",
    "        Appends the dataSample dataframe with the t-SNE X and Y coordinates\n",
    "        Returns tsneX and tsneY\n",
    "        \"\"\"\n",
    "        scaler = preprocessing.StandardScaler().fit(self.dataSample)\n",
    "        scaledData = scaler.transform(self.dataSample)\n",
    "        tsne = TSNE(n_components=2,perplexity=perplexity,init='pca',verbose=True)\n",
    "        tsne_fit=tsne.fit_transform(scaledData)\n",
    "        self.dataSample['tsne_x'] = tsne_fit.T[0]\n",
    "        self.dataSample['tsne_y'] = tsne_fit.T[1]\n",
    "        # Goal is to minimize the KL-Divergence\n",
    "        if sklearn.__version__ == '0.18.1':\n",
    "            print(\"KL-Divergence was %s\"%tsne.kl_divergence_ )\n",
    "            \n",
    "        return tsne_fit.T[0],tsne_fit.T[1]\n",
    "    \n",
    "    def km_out(self):\n",
    "        clusterLabels = km_outliers.kmeans_w_outliers(\n",
    "            self.filesSample,self.dataSample[['tsne_x','tsne_y']],1)\n",
    "        self.dataSample['km_cluster']=clusterLabels\n",
    "        \n",
    "    def db_out(self):\n",
    "        clusterLabels = db_outliers.dbscan_w_outliers(\n",
    "            self.filesSample,self.dataSample[['tsne_x','tsne_y']])\n",
    "        self.dataSample['db_cluster']=clusterLabels\n",
    "        \n",
    "    def save(self,of):\n",
    "        data.to_csv(of)\n",
    "        \n",
    "Q8 = feature_importer(filelist,fitsDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random sampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q8.randSampleWTabby(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply t-SNE dimensionallity reduction and plot for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = Q8.tsne_fit()\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    sns.kdeplot(x, y,shade=False,cmap=\"nipy_spectral\")\n",
    "    plt.scatter(x, y,alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = Q8.dataSample.index #hmm, needs to be put into the class?\n",
    "data = Q8.dataSample[['tsne_x','tsne_y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Creates clusters from processed data for any number of dimensions using k-means or DBSCAN.\n",
    "\n",
    "Requirement: Processed lightcurve data in a .pkl file. Elements in .pkl file should be\n",
    "arranged as follows:\n",
    "[str(filename),feature1,feature2,...,featureN]\n",
    "\"\"\"\n",
    "\n",
    "root = Tk.Tk()\n",
    "def reorganizeArray(X): \n",
    "    # This is just a transposition, could be accomplished by making X an np.array then transposing via X.T if prefered\n",
    "    # TODO: Time the difference with large datasets (100,000+)\n",
    "    return [[X[i][j] for i in range(len(X))] for j in range(len(X[0]))]\n",
    "\n",
    "def loadData(filename):\n",
    "    pathtofile = os.path.dirname(filename)\n",
    "    f = open(filename,'r+') # show an \"Open\" dialog box and return the path to the selected file\n",
    "    data = []\n",
    "    while True:\n",
    "        try:\n",
    "            o = pickle.load(f)\n",
    "        except EOFError:\n",
    "            break\n",
    "        else:\n",
    "            data.append(o)\n",
    "    f.close()\n",
    "    \n",
    "    if len(data) == 1:\n",
    "        data = data[0]\n",
    "        \n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "DBSCAN Clustering\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "\n",
    "def eps_est(data):\n",
    "    # distance array containing all distances\n",
    "    nbrs = NearestNeighbors(n_neighbors=1000, algorithm='ball_tree').fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    # Distance to 200th nearest neighbor, using 200th instead of 4th because: ... reasons\n",
    "    distArr = distances[:,100]\n",
    "    distArr.sort()\n",
    "    pts = range(len(distArr))\n",
    "\n",
    "    # The following looks for the first instance (past the mid point) where the mean of the following 50 points\n",
    "    # is at least 5% greater than the mean of the previous 50 points.\n",
    "    # Alternatively, perhaps a better method, we could consider the variance of the points and draw conclusions from that\n",
    "    number = 50\n",
    "    cutoff = 1.05\n",
    "    for i in range(number,len(pts)-number):\n",
    "        if np.mean(distArr[i+1:i+number])>=cutoff*np.mean(distArr[i-number:i-1]) and i>(len(pts)%2+len(pts))/2:\n",
    "\n",
    "            dbEps = distArr[i]\n",
    "            break\n",
    "        \n",
    "    # Estimating nneighbors by finding the number of pts. \n",
    "    # that fall w/in our determined eps for each point.\n",
    "\n",
    "    count = np.zeros(len(pts))\n",
    "    for i in pts:    \n",
    "        for dist in distances[i]:\n",
    "            if dist <= dbEps:\n",
    "                count[i]+=1\n",
    "    average = np.median(count)\n",
    "    sigma = np.std(count)\n",
    "    neighbors = average/2 # Divide by 2 for pts on the edges\n",
    "    print(\"\"\"\n",
    "    Epsilon is in the neighborhood of %s, \n",
    "    with an average of %s neighbors within epsilon,\n",
    "    %s neighbors in half circle (neighbors/2).\n",
    "    \"\"\"%(dbEps,average,neighbors))\n",
    "    return dbEps,neighbors\n",
    "\n",
    "\n",
    "\n",
    "def DBSCAN_clusters(data,eps,min_points=None):\n",
    "    npdata = np.array(data)\n",
    "    if min_points==None:\n",
    "        est = DBSCAN(eps=eps)\n",
    "    else:\n",
    "        est = DBSCAN(eps=eps,min_samples=min_points)\n",
    "    \n",
    "    est.fit(npdata)\n",
    "    clusters = est.labels_\n",
    "    coreSampleIndices = est.core_sample_indices_\n",
    "    \n",
    "    return clusters, coreSampleIndices\n",
    "\n",
    "def DBSCAN_cluster(files,data):\n",
    "    dbEps,neighbors= eps_est(data)\n",
    "    params = [dbEps,neighbors]\n",
    "    print(\"Clustering data with DBSCAN...\")\n",
    "    clusterLabels,coreSampleIndices = DBSCAN_clusters(data,dbEps,neighbors)\n",
    "    plotArray = [files,clusterLabels,data]\n",
    "    print(\"Sorting...\")\n",
    "    centerIndex=coreSampleIndices[0]\n",
    "    outlierIndices = [i for i in range(len(clusterLabels))if clusterLabels[i]==-1]\n",
    "    outlierFiles = [files[i] for i in outlierIndices]\n",
    "    numout = len(outlierFiles)\n",
    "    numclusters = max(clusterLabels+1)\n",
    "    tabbyInd = files.index(fnmatch.filter(files,'*8462852*')[0])\n",
    "    if fnmatch.filter(files,'*8462852*')[0] in outlierFiles:\n",
    "        print(\"Tabby has been found to be an outlier in DBSCAN.\")\n",
    "    else:\n",
    "        print(\"Tabby has not been found to be an outlier in DBSCAN\")\n",
    "        \n",
    "    print(\"There were %s clusters and %s total outliers\"%(numclusters,numout))\n",
    "    return clusterLabels,coreSampleIndices, params\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "K-means Clustering\n",
    "\"\"\"\n",
    "\n",
    "def KMeans_clusters(data,nclusters):\n",
    "    # Run KMeans, get clusters\n",
    "    npdata = np.array(data)\n",
    "    est = KMeans(n_clusters=nclusters)\n",
    "    est.fit(npdata)\n",
    "    clusters = est.labels_\n",
    "    centers = est.cluster_centers_\n",
    "    return clusters, centers\n",
    "\n",
    "def outliers(data,clusters,centers,files):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    data - all the data\n",
    "    clusters - the cluster labels from kmeans.\n",
    "    centers - locations of the cluster centers\n",
    "    \n",
    "    Purpose:\n",
    "    Separate out the data on the edge of the clusters which are the most likely outliers.\n",
    "    \"\"\"\n",
    "    # Will try to stick to the following:\n",
    "    # cluster i, lightcurve j, feature k\n",
    "    nclusters = len(centers)\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializing arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    dataByCluster = []\n",
    "    clusterIndices = []\n",
    "    clusters = np.array(clusters)\n",
    "    \n",
    "    for i in range(min(clusters),max(clusters)+1):\n",
    "        # Keeping track of which points get pulled into each cluster:\n",
    "        clusterIndices.append([j[0] for j in enumerate(clusters) if j[1]==i])\n",
    "        \n",
    "        # Separating the cluster data out into their own arrays (w/in the cluster array)\n",
    "        dataByCluster.append([data[j[1]] for j in enumerate(clusterIndices[i])])\n",
    "            \n",
    "    allTypical=[]\n",
    "    allOutliers=[]\n",
    "\n",
    "    for i in range(nclusters):\n",
    "        \"\"\"\n",
    "        ========== Finding points outside of the cutoff ===========\n",
    "        \"\"\"\n",
    "        sigma = np.std(dataByCluster[i])\n",
    "        cutoff = 2*sigma\n",
    "        \"\"\"\n",
    "            ==== Calculating distances to each point ====\n",
    "        \"\"\"\n",
    "        # Calculate distances to each point in each cluster to the center of its cluster\n",
    "        distFromCenter=[sum((pt-centers[i])**2)**.5 for pt in dataByCluster[i]]\n",
    "        \n",
    "        \"\"\"\n",
    "            ==== Finding outliers and the standard (defined by the closest to the center) ====\n",
    "        \"\"\"\n",
    "        \n",
    "        # returns cluster indices of the outliers and the typical lcs\n",
    "        outliers=[j[0] for j in enumerate(distFromCenter) if j[1]>=cutoff]\n",
    "        typical = [j[0] for j in enumerate(distFromCenter) if j[1]==min(distFromCenter)]\n",
    "\n",
    "        clusters[outliers]=-1\n",
    "        clusters[typical]=0\n",
    "        \n",
    "    return clusters\n",
    "\n",
    "\"\"\"\n",
    "============ Start ============\n",
    "\"\"\"\n",
    "\n",
    "def kmeans_w_outliers(files,data,nclusters=1):\n",
    "    # nclusters can be obtained through the optimalK.py script \n",
    "\n",
    "    clusters,centers=KMeans_clusters(data,nclusters)\n",
    "        \n",
    "    clusterLabels=outliers(data,clusters,centers,files)\n",
    "    clusterLabels = np.array(clusterLabels)\n",
    "    \n",
    "    data['cluster']=clusterLabels\n",
    "    numout = data[data.cluster==-1].cluster.count()\n",
    "    if data.index.str.contains('8462852').any():\n",
    "        if data['cluster'].loc[data.index.str.contains('8462852')] == -1:\n",
    "            print(\"Tabby has been found to be an outlier in k-means.\")\n",
    "        else:\n",
    "            print(\"Tabby has NOT been found to be an outlier in k-means\")\n",
    "\n",
    "    print(\"There were %s outliers in %s clusters\"%(numout,nclusters))\n",
    "    \n",
    "    \n",
    "    outlierfiles = data[data.cluster==-1].index\n",
    "    plotArray = [files,clusterLabels,data]\n",
    "    print(\"Done.\")\n",
    "        \n",
    "def dbscan_w_outliers(files,data):\n",
    "    print(\"Clustering data...\")\n",
    "    clusterLabels,coreSampleIndices,params = DBSCAN_cluster(files,data)\n",
    "    plotArray = [files,clusterLabels,data]\n",
    "    print(\"Sorting...\")\n",
    "    centerIndex=coreSampleIndices[0]\n",
    "    outlierfiles = [files[i[0]] for i in enumerate(clusterLabels) if i[1]==-1]\n",
    "    print(\"Done.\")\n",
    "\n",
    "kmeans_w_outliers(files,data,1)\n",
    "dbscan_w_outliers(files,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot clusters (must be generated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = Tk.Tk()\n",
    "root.wm_title(\"Scatter\")\n",
    "\n",
    "def read_kepler_curve(file):\n",
    "    lc = pyfits.getdata(file)\n",
    "    t = lc.field('TIME')\n",
    "    f = lc.field('PDCSAP_FLUX')\n",
    "    err = lc.field('PDCSAP_FLUX_ERR')\n",
    "    f = f[np.isfinite(t)]\n",
    "    t = t[np.isfinite(t)]\n",
    "    t = t[np.isfinite(f)]\n",
    "    f = f[np.isfinite(f)]\n",
    "    \n",
    "    nf = f / np.median(f)\n",
    "\n",
    "    return t, nf, err\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"--- import light curve data ---\"\"\"\n",
    "    pathtofits = '/home/dgiles/Documents/KeplerLCs/fitsFiles/Q8fitsfiles/'\n",
    "    \n",
    "    # The following needs to be generated in the cell above.\n",
    "    files = plotArray[0]\n",
    "    clusterLabels = plotArray[1]\n",
    "    # data is an array containing each data point\n",
    "    data = np.array(plotArray[2])\n",
    "    \n",
    "    cNorm  = colors.Normalize(vmin=0, vmax=max(clusterLabels))\n",
    "    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap='jet')\n",
    "\n",
    "    # tsneX has all the x-coordinates\n",
    "    tsneX = data[:,0]\n",
    "    # tsneY has all the y-coordinates\n",
    "    tsneY = data[:,1]   \n",
    "    outX = []\n",
    "    outY = []\n",
    "    files_out = []\n",
    "    clusterX = []\n",
    "    clusterY = []\n",
    "    files_cluster = []\n",
    "    \n",
    "    for i in enumerate(data):\n",
    "        if clusterLabels[i[0]] == -1:\n",
    "            outX.append(i[1][0])\n",
    "            outY.append(i[1][1])\n",
    "            files_out.append(files[i[0]])\n",
    "        else:\n",
    "            clusterX.append(i[1][0])\n",
    "            clusterY.append(i[1][1])\n",
    "            files_cluster.append(files[i[0]])\n",
    "    \n",
    "    filesWpath = [pathtofits+files[i] for i in range(len(files))]\n",
    "    numcpus = cpu_count()\n",
    "    usecpus = numcpus*4\n",
    "    p = Pool(usecpus)\n",
    "    lc = p.map(read_kepler_curve,filesWpath)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "    t=[lc[i][0] for i in range(len(lc))]\n",
    "    nf=[lc[i][1] for i in range(len(lc))]\n",
    "\n",
    "    lightcurveData=np.array([t,nf])\n",
    "    \n",
    "    del(lc)\n",
    "    del(t)\n",
    "    del(nf)\n",
    "    del(numcpus)\n",
    "    del(usecpus)\n",
    "    del(filesWpath)\n",
    "    \"\"\"--- Organizing data and Labels ---\"\"\"\n",
    "\n",
    "    tabbyInd = files.index(fnmatch.filter(files,'*8462852*')[0])\n",
    "    \n",
    "    fig = Figure(figsize=(30,15))\n",
    "    \n",
    "    \n",
    "    # a tk.DrawingArea\n",
    "    canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "    canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n",
    "    # Toolbar to help navigate the data (pan, zoom, save image, etc.)\n",
    "    toolbar = NavigationToolbar2TkAgg(canvas, root)\n",
    "    toolbar.update()\n",
    "    canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n",
    "    \n",
    "    gs = gridspec.GridSpec(2,6)\n",
    "    \n",
    "    with sns.axes_style(\"white\"):\n",
    "        # empty subplot for scattered data\n",
    "        ax = fig.add_subplot(gs[0,:4])\n",
    "        # empty subplot for lightcurves\n",
    "        ax2 = fig.add_subplot(gs[1,:])\n",
    "        # empty subplot for center detail\n",
    "        ax3 = fig.add_subplot(gs[0,4:])\n",
    "    \n",
    "    def distance(point, event):\n",
    "        \"\"\"Return distance between mouse position and given data point\n",
    "\n",
    "        Args:\n",
    "            point (np.array): np.array of shape (3,), with x,y,z in data coords\n",
    "            event (MouseEvent): mouse event (which contains mouse position in .x and .xdata)\n",
    "        Returns:\n",
    "            distance (np.float64): distance (in screen coords) between mouse pos and data point\n",
    "        \"\"\"\n",
    "        assert point.shape == (2,), \"distance: point.shape is wrong: %s, must be (2,)\" % point.shape\n",
    "        x2,y2 = ax.transData.transform((point[0],point[1]))\n",
    "\n",
    "        return np.sqrt ((x2 - event.x)**2 + (y2 - event.y)**2)\n",
    "    \n",
    "    def calcClosestDatapoint(XT, event):\n",
    "        \"\"\"Calculate which data point is closest to the mouse position.\n",
    "        \n",
    "        Args:\n",
    "            XT (np.array) - array of points, of shape (numPoints, 2)\n",
    "            event (MouseEvent) - mouse event (containing mouse position)\n",
    "        Returns:\n",
    "            smallestIndex (int) - the index (into the array of points X) of the element closest to the mouse position\n",
    "        \"\"\"\n",
    "        distances = [distance (XT[:,i], event) for i in range(XT.shape[1])]\n",
    "        \n",
    "        return np.argmin(distances)\n",
    "    \n",
    "    def drawData(X, index):\n",
    "        # Plots the lightcurve of the point chosen\n",
    "        ax2.cla()\n",
    "        \n",
    "        x=X[0][index]\n",
    "        y=X[1][index]\n",
    "        \n",
    "        axrange=0.55*(max(y)-min(y))\n",
    "        mid=(max(y)+min(y))/2\n",
    "        yaxmin = mid-axrange\n",
    "        yaxmax = mid+axrange\n",
    "        if yaxmin < .95:\n",
    "            if yaxmax > 1.05:\n",
    "                ax2.set_ylim(yaxmin,yaxmax)\n",
    "            else:\n",
    "                ax2.set_ylim(yaxmin,1.05)\n",
    "        elif yaxmax > 1.05:\n",
    "            ax2.set_ylim(.95,yaxmax)\n",
    "        else:\n",
    "            ax2.set_ylim(.95,1.05)\n",
    "\n",
    "        if files[index] in files_cluster:\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        ax2.plot(x, y, 'o',markeredgecolor='none', c=color, alpha=0.2)\n",
    "        ax2.plot(x, y, '-',markeredgecolor='none', c=color, alpha=0.7)\n",
    "        #ax2.set_title(files[index][:13],fontsize = 20)\n",
    "        ax2.set_xlabel('Time (Days)',fontsize=22)\n",
    "        ax2.set_ylabel(r'$\\frac{\\Delta F}{F}$',fontsize=30)\n",
    "        \n",
    "        fig.suptitle(files[index][:13],fontsize=30)\n",
    "        \n",
    "        canvas.draw()\n",
    "        \n",
    "    def annotatePt(XT, index):\n",
    "        \"\"\"Create popover label in 3d chart\n",
    "\n",
    "        Args:\n",
    "            X (np.array) - array of points, of shape (numPoints, 3)\n",
    "            index (int) - index (into points array X) of item which should be printed\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        x2, y2 = XT[index][0], XT[index][1]\n",
    "        # Either update the position, or create the annotation\n",
    "        if hasattr(annotatePt, 'label'):\n",
    "            annotatePt.label.remove()\n",
    "            annotatePt.emph.remove()\n",
    "        if hasattr(annotatePt, 'emphCD'):\n",
    "            annotatePt.emphCD.remove()\n",
    "\n",
    "        # Get data point from array of points X, at position index\n",
    "        annotatePt.label = ax.annotate( \"\",\n",
    "            xy = (x2, y2), xytext = (x2+10, y2+10),\n",
    "            arrowprops = dict(headlength=20,headwidth=20,width=6,shrink=.1,color='red'))\n",
    "        annotatePt.emph = ax.scatter(x2,y2,marker='o',s=50,c='red')\n",
    "        if files[index] in files_cluster:\n",
    "            annotatePt.emphCD = ax3.scatter(x2,y2,marker='o',s=150,c='red')\n",
    "        else:\n",
    "            annotatePt.emphCD = ax.scatter(x2,y2,marker='o',s=50,c='red')\n",
    "        canvas.draw()\n",
    "    \n",
    "    \n",
    "    def onMouseClick(event, X):\n",
    "        \"\"\"Event that is triggered when mouse is clicked. Shows lightcurve for data point closest to mouse.\"\"\"\n",
    "        XT = np.array(reorganizeArray(X)) # array organized by feature, each in it's own array\n",
    "        closestIndex = calcClosestDatapoint(XT, event)\n",
    "        drawData(lightcurveData, closestIndex)\n",
    "        \n",
    "    def onMouseRelease(event, X):\n",
    "        XT = np.array(reorganizeArray(X))\n",
    "        closestIndex = calcClosestDatapoint(XT, event)\n",
    "        annotatePt(X,closestIndex)\n",
    "        #for centerIndex in centerIndices:\n",
    "        #    annotateCenter(XT,centerIndex)\n",
    "    \n",
    "    def connect(X):\n",
    "        if hasattr(connect,'cidpress'):\n",
    "            fig.canvas.mpl_disconnect(connect.cidpress)\n",
    "        if hasattr(connect,'cidrelease'):\n",
    "            fig.canvas.mpl_disconnect(connect.cidrelease)\n",
    "            \n",
    "        connect.cidpress = fig.canvas.mpl_connect('button_press_event', lambda event: onMouseClick(event,X))\n",
    "        connect.cidrelease = fig.canvas.mpl_connect('button_release_event', lambda event: onMouseRelease(event, X))\n",
    "    \n",
    "    def redraw():       \n",
    "        # Clear the existing plots\n",
    "        ax.cla()\n",
    "        ax2.cla()\n",
    "        ax3.cla()\n",
    "        # Set those labels\n",
    "        ax.set_xlabel(\"T-SNE X\",fontsize=18)\n",
    "        ax.set_ylabel(\"T-SNE Y\",fontsize=18)\n",
    "        # Scatter the data\n",
    "        ax.scatter(outX, outY,c=\"black\",s=30,cmap='jet')\n",
    "        ax.hexbin(clusterX,clusterY,mincnt=5,bins=\"log\",cmap=\"inferno\",gridsize=35)\n",
    "        \n",
    "        hb = ax3.hexbin(clusterX,clusterY,mincnt=5,bins=\"log\",cmap=\"inferno\",gridsize=35)\n",
    "        cb = fig.colorbar(hb)\n",
    "        ax3.set_title(\"Center Density Detail\")\n",
    "        ax3.set_xlabel(\"T-SNE X\",fontsize=18)\n",
    "        ax3.set_ylabel(\"T-SNE Y\",fontsize=18)\n",
    "        \n",
    "        \n",
    "        #for centerIndex in centerIndices:\n",
    "        #    annotateCenter(currentData1,centerIndex)\n",
    "        \n",
    "        if hasattr(redraw,'cidenter'):\n",
    "                fig.canvas.mpl_disconnect(redraw.cidenter)\n",
    "                fig.canvas.mpl_disconnect(redraw.cidexit)\n",
    "        connect(data)\n",
    "            \n",
    "        annotatePt(data,tabbyInd)\n",
    "        drawData(lightcurveData,tabbyInd)\n",
    "        #fig.savefig('Plots/Q16_PCA_kmeans/Tabby.png')\n",
    "        canvas.draw()\n",
    "        \n",
    "    print(\"Plotting.\")\n",
    "    \n",
    "    redraw() # First draw, Tabby plotted\n",
    "    canvas.show()\n",
    "    \n",
    "    def quit():\n",
    "        print(\"Exitting.\")\n",
    "        root.quit()\n",
    "        root.destroy()\n",
    "        \n",
    "    Tk.Button(root, text=\"Quit\", command=quit).pack()\n",
    "    \n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "outX = []\n",
    "outY = []\n",
    "files_out = []\n",
    "clusterX = []\n",
    "clusterY = []\n",
    "files_cluster = []\n",
    "clusteredLabels = []\n",
    "\n",
    "for i in enumerate(data):\n",
    "    if clusterLabels[i[0]] == -1:\n",
    "        outX.append(i[1][0])\n",
    "        outY.append(i[1][1])\n",
    "        files_out.append(files[i[0]])\n",
    "    else:\n",
    "        clusterX.append(i[1][0])\n",
    "        clusterY.append(i[1][1])\n",
    "        files_cluster.append(files[i[0]])\n",
    "        clusteredLabels.append(clusterLabels[i[0]])\n",
    "        \n",
    "df = pd.DataFrame()\n",
    "df['t-SNE_X'] = clusterX\n",
    "df['t-SNE_Y'] = clusterY\n",
    "\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=max(clusteredLabels))\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap='jet')\n",
    "           \n",
    "with sns.axes_style(\"white\"):\n",
    "\n",
    "    plt.xlabel(\"t-SNE X\",fontsize=18)\n",
    "    plt.ylabel(\"t-SNE Y\",fontsize=18)\n",
    "    #plt.hexbin(clusterX,clusterY,mincnt=1,bins=\"log\",cmap=\"inferno\",gridsize=35)\n",
    "    #plt.colorbar()\n",
    "    plt.scatter(clusterX,clusterY,marker='o',c=scalarMap.to_rgba(clusteredLabels))\n",
    "    plt.scatter(outX,outY,c=\"red\")\n",
    "    xmid = (max(clusterX)+min(clusterX))/2\n",
    "    xrng = (max(clusterX)-min(clusterX))/2\n",
    "    ymid = (max(clusterY)+min(clusterY))/2\n",
    "    yrng = (max(clusterY)-min(clusterY))/2\n",
    "    plt.scatter(data[tabbyInd][0],data[tabbyInd][1],c='green',s=50)\n",
    "    plt.xlim(xmid-xrng*1.05,xmid+xrng*1.05)\n",
    "    plt.ylim(ymid-yrng*1.05,ymid+yrng*1.05)\n",
    "    g = sns.jointplot(x=\"t-SNE_X\",y=\"t-SNE_Y\",data=df,kind='scatter',color=\"black\",stat_func=None,size=10,\\\n",
    "                      xlim=(xmid-xrng*1.05,xmid+xrng*1.05),ylim=(ymid-yrng*1.05,ymid+yrng*1.05),\\\n",
    "                      marginal_kws=dict(bins=200),\\\n",
    "                      joint_kws=dict(s=1))\n",
    "    \n",
    "    g.set_axis_labels(\"t-SNE X\",\"t-SNE Y\", fontsize=18)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
